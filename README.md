# MachineLearning_HomeAssignment5

University of Central Missouri

Student Information

Name: CHINTALA SAI SRAVAN
Student ID: 700773836
Course: CS5710 â€” Machine Learning
Semester: Fall 2025
Assignment: Home Assignment 5

ğŸ“Œ Overview

This repository contains my solutions for Home Assignment 5, which covers two main components:

Part A â€” Short-Answer Questions
Focused on Transformers, Positional Encoding, Attention Mechanisms, Multi-Head Attention, Ethics in AI, Dataset Bias, and AI-related Harms.

Part B â€” Coding Tasks

Q1: Implementation of Scaled Dot-Product Attention using NumPy

Q2: Implementation of a Simple Transformer Encoder Block using PyTorch

Includes Multi-Head Self-Attention, Feed-Forward Network, LayerNorm, Residuals, and Shape Verification

All code is fully commented as required and structured for clarity.

ğŸ“ Part A â€” Short Answer Summary

The written answers include detailed explanations on:

âœ” Positional Encoding

Why Transformers need positional encodings

Requirements for good encoding schemes

Unitary & norm-preserving positional matrices

âœ” Attention Mechanism

Definition of attention scores

Role of softmax

Computation of context vectors

âœ” Multi-Head Attention

Advantages of multiple heads

Subspace projection

Why concatenation + linear projection is needed

âœ” Ethics in AI

Difference between ethics, laws, and feelings

Utilitarian vs Deontological reasoning in AI decisions

Why no single ethical theory dominates

âœ” AI Harms

Allocational vs representational harms

Real-world examples

Why representation harms are harder to measure

âœ” Dataset Bias

Sources of bias

Underrepresented groups

Bias amplification

âœ” Security & Privacy

Data poisoning

Model memorization

Model stealing

ğŸ’» Part B â€” Coding
Q1 â€” Scaled Dot-Product Attention (NumPy)

This script implements:

Stable softmax

Scaled dot-product attention

Optional masking

Test demonstrating correct output shapes

Q2 â€” Transformer Encoder Block (PyTorch)

This script includes:

Multi-Head Self-Attention implemented manually

Feed-Forward Network

Add & Norm layers

Residual connections

Dropout

Shape verification test

âš™ï¸ Dependencies

Ensure you have the following installed:

Python Libraries
pip install numpy torch

Versions used during development:

Python 3.10

NumPy 1.26+

PyTorch 2.0+

ğŸ“¤ Submission Notes

âœ” Code is fully commented
âœ” Student details included
âœ” README thoroughly explains all components
âœ” GitHub repo contains both Part A and Part B
âœ” Output tests provided
